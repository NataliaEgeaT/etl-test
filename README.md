# ETL Technical Test â€“ Data Engineer
This project implements a reproducible ETL pipeline using **Python + Pandas + DuckDB**, delivering a complete **raw** and **curated** data processing workflow, following all mandatory requirements from the technical test.

# ðŸš€ Features

- âœ… Locally (Python + DuckDB warehouse)  
- âœ… Inside Docker (MSSQL + ETL)  
- Full architecture and technical decisions are documented in:

ðŸ“„ `docs/design_notes.md`

# Project Structure

```
etl-test/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ run_checks.sh
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ etl_job.py
â”‚  â”œâ”€ api_client.py
â”‚  â”œâ”€ transforms.py
â”‚  â”œâ”€ db.py
|  â”œâ”€ mssql_client.py 
â”‚  â””â”€ utils.py
â”‚
â”œâ”€ sample_data/
â”‚  â”œâ”€ api_orders.json
â”‚
â”œâ”€ output/
â”‚  â”œâ”€ raw/
â”‚  â””â”€ curated/
â”‚
â”œâ”€ sql/
|  â”œâ”€ init.sql
â”‚  â””â”€ redshift-ddl.sql
â”‚
â”œâ”€ tests/
â”‚  â””â”€ test_transforms.py
â”‚
â””â”€ docs/
   â””â”€ design_notes.md
```

# How to Run the Project Locally

## 1. Create a virtual environment
Linux/Mac:
```
python3 -m venv venv
source venv/bin/activate
```

Windows:
```
python -m venv venv
venv\Scripts\activate
```

## 2. Install dependencies
```
pip install -r requirements.txt
```

## 3. Run the ETL
Full load:
```
python -m src.etl_job
```

Incremental load:
```
python -m src.etl_job --since "2025-01-01T00:00:00Z"
```

Outputs:
output/raw/
output/curated/date=YYYY-MM-DD/

# Run the Project Using Docker

Build:
```
docker-compose build
```

Run ETL:
```
docker-compose run --rm app python -m src.etl_job
```

Incremental:
```
docker-compose run --rm app python -m src.etl_job --since "2025-01-01T00:00:00Z"
```

# Run Tests
```
PYTHONPATH=. pytest -q
```

# Run all checks

```
./run_checks.sh
```

# SQL â€“ Warehouse Tables
DDL located at:
sql/redshift-ddl.sql

Includes:
- dim_user  
- dim_product  
- fact_order  

# Documentation
See docs/design_notes.md

# Notes
- No credentials in repo.
- ETL is idempotent.
- Docker optional.
